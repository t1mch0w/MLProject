\section{Experiments}
In this section, we will introduce the data collection, data sets, and how we conduct the experiments.
\subsection{Data Collection}
All the data comes from Dotabuff, which is the largest third-party Dota2 game track website.
In the team section of Dotabuff, there are more than 1,000 teams sharing their game results.
We write a crawler to collect the game data by Python urllib2 and Beautifulsoup libraries.
As Section IV mentioned, from each game, we can get the 111 hero usage, 30 synergic pairs of heroes usage,
30 conter pairs of heroes usage, game running time, and the final result.
We kept running the crawler for four days to collect data.
\subsection{Data Sets}
Our data sets include 15,197 Dota2 matches records from top 100 win-rate teams in the world.
Each feature vector includes 111 hero feature, 30 synergic pair feature, 30 conter pair feature,
1 game time feature, and 1 game result feature.
In total, there are 173 features in the feature vector.

\subsection{Training and Testing}
We use 10-fold cross-validation as our training and testing methods.
The original data sets are randomly partitioned into 10 equal sized subsets, use one of the subsets as the test set and the others as the testing set.
The process is to repeate 10 folds, with each subset used exactly once as the testing set.
The results are the combined from 10 folds.

\subsection{Evaluation Metrics}
F-score is used as the evaluation metric our experiment, since both the precision and recall are considered by this method.
F-score can be calculated by the following formulas.

\subsection{Decision Tree}
\subsection{Logistic Regression}
\subsection{Neural Network}
\subsection{SVM}

\begin{equation}
F=2\times\frac{Precision+Recall}{Precision*Recall}
\end{equation}

\begin{equation}
Precision = \frac{\#TruePositives}{\#TruePositives+\#FalsePositives}
\end{equation}

\begin{equation}
Recall = \frac{\#TruePositives}{\#TruePositives+\#FalseNegatives}
\end{equation}

We also use Confusion Matrix to present our results.
A sample form is described in Table~\ref{table:sample}.
There are 200 samples in the testing data set.
200 instances are classified correctly, and 100 instances are classified wrongly.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Predicted: True & Predicted: False & N = 200 \\ \hline
100 & 50 & Actual: True \\ \hline
50 & 100 & Actual: False \\ \hline
\end{tabular}
\caption{A Sample of Confusion Matrix}
\label{table:sample}
\end{center}
\end{table}

\subsection{Results}
Due to 10-fold cross-validation, every instance in the data sets has been used as test cases.
There are 15,197 samples in total.
9,247 instances (60.8475\%) have been classified correctly, and 5,950 instances (39.1525\%) have been classified incorrectly.
The accuracy by class and confusion matrix can be seen in Table~\ref{table:accuracy} and Table~\ref{table:matrix}.


\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Class & Precision & Recall & F-score \\ \hline
0 & 0.609 & 0.608 & 0.654 \\ \hline
1 & 0.608 & 0.609 & 0.654 \\ \hline
\end{tabular}
\caption{Detailed Accuracy by Class}
\label{table:accuracy}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
a & b & \\ \hline
4,620 & 2,978 & a = 0 \\ \hline
2,972 & 4,627 & b = 1 \\ \hline
\end{tabular}
\caption{Confusion Matrix}
\label{table:matrix}
\end{center}
\end{table}

\subsection{Results from Other Models}
We also applied other models to the problem.
 The results are shown in Table~\ref{table:others}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
\hline
Model & F-score \\ \hline
Logistic Regression & 54.08\% \\ \hline
Natural Network & 59.51\% \\ \hline
Support Vector Machine & 60.43\% \\ \hline
Native Bayes & 58.92\% \\ \hline
\end{tabular}
\caption{Results from Other Models}
\label{table:others}
\end{center}
\end{table}
